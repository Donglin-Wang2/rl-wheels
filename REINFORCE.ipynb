{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "env_name=\"CartPole-v1\"\n",
    "env = gym.make(env_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env) -> None:\n",
    "        super().__init__()\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, 100),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(100, self.action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        return self.net(state)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "## Casual Ablation:\n",
    "## 1. The use of torch.distribution helped the gradient calculation\n",
    "## 2. Use of Adam instead of SGD helped with the learning\n",
    "\n",
    "def grad_descent(actions, rewards):\n",
    "    G = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        G.insert(0, R)\n",
    "    for i in range(len(G)):\n",
    "        G[i] *= -actions[i]\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.cat(G).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def rand_action():\n",
    "    return np.random.choice(range(2), p=[0.5, 0.5])\n",
    "\n",
    "# def sample_action(prob):\n",
    "#     dist = Categorical(prob)\n",
    "#     action = dist.sample()\n",
    "#     log_probs.append(dist.log_prob(action))\n",
    "#     return action.item()\n",
    "\n",
    "def sample_action(prob):\n",
    "    action = np.random.choice(range(pol_net.action_dim), p=prob.squeeze(0).detach().numpy())\n",
    "    log_probs.append(torch.log(prob[0][action]).unsqueeze(0))\n",
    "    return action\n",
    "\n",
    "num_epi = 300\n",
    "pol_net = PolicyNet(env)\n",
    "\n",
    "gamma = 0.99\n",
    "optimizer = optim.Adam(pol_net.parameters(), lr = 1e-3)\n",
    "log_freq = 10\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_epi):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    log_probs, rewards = [], []\n",
    "    for t in range(20000):\n",
    "        old_state = state\n",
    "        action_prob = pol_net(torch.tensor(state).unsqueeze(0))\n",
    "        action = sample_action(action_prob)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            grad_descent(log_probs, rewards)\n",
    "            break\n",
    "    if i % log_freq == 0:\n",
    "        print('Total Reward: %d' % sum(rewards))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/s6/qk66mxfn1jqgsjslnssw3mjm0000gn/T/ipykernel_93496/155788214.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s6/qk66mxfn1jqgsjslnssw3mjm0000gn/T/ipykernel_93496/3473835596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mgrad_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s6/qk66mxfn1jqgsjslnssw3mjm0000gn/T/ipykernel_93496/3473835596.py\u001b[0m in \u001b[0;36mgrad_descent\u001b[0;34m(actions, rewards)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "state = env.reset()\n",
    "for t in range(20000):\n",
    "    env.render()\n",
    "    old_state = state\n",
    "    action_prob = pol_net(torch.tensor(state).unsqueeze(0))\n",
    "    action = sample_action(action_prob)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('stable-gyms': conda)"
  },
  "interpreter": {
   "hash": "4a2d71ed0b2247ad884396d1bec5de2549d25faa038972ff93b566375f2d796f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}